{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\github_clone\\\\Deep-Inverse-Reinforcement-Learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IRL import PPO\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training environment name : LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action space dimension\n",
    "if has_continuous_action_space:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current logging run number for LunarLander-v2 :  0\n",
      "logging at : PPO_logs/LunarLander-v2//PPO_LunarLander-v2_log_0.csv\n"
     ]
    }
   ],
   "source": [
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save checkpoint path : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n"
     ]
    }
   ],
   "source": [
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training at (GMT) :  2024-08-06 20:36:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ernest\\.conda\\envs\\l2rpn-test\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 17 \t\t Timestep : 1600 \t\t Average Reward : -163.36\n",
      "Episode : 34 \t\t Timestep : 3200 \t\t Average Reward : -128.01\n",
      "Episode : 53 \t\t Timestep : 4800 \t\t Average Reward : -151.8\n",
      "Episode : 72 \t\t Timestep : 6400 \t\t Average Reward : -138.29\n",
      "Episode : 88 \t\t Timestep : 8000 \t\t Average Reward : -174.23\n",
      "Episode : 105 \t\t Timestep : 9600 \t\t Average Reward : -195.23\n",
      "Episode : 124 \t\t Timestep : 11200 \t\t Average Reward : -159.34\n",
      "Episode : 141 \t\t Timestep : 12800 \t\t Average Reward : -167.01\n",
      "Episode : 155 \t\t Timestep : 14400 \t\t Average Reward : -99.92\n",
      "Episode : 174 \t\t Timestep : 16000 \t\t Average Reward : -110.32\n",
      "Episode : 193 \t\t Timestep : 17600 \t\t Average Reward : -110.16\n",
      "Episode : 212 \t\t Timestep : 19200 \t\t Average Reward : -118.54\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:00:35\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 227 \t\t Timestep : 20800 \t\t Average Reward : -96.63\n",
      "Episode : 243 \t\t Timestep : 22400 \t\t Average Reward : -75.76\n",
      "Episode : 258 \t\t Timestep : 24000 \t\t Average Reward : -101.47\n",
      "Episode : 274 \t\t Timestep : 25600 \t\t Average Reward : -85.8\n",
      "Episode : 285 \t\t Timestep : 27200 \t\t Average Reward : -75.3\n",
      "Episode : 297 \t\t Timestep : 28800 \t\t Average Reward : -33.75\n",
      "Episode : 308 \t\t Timestep : 30400 \t\t Average Reward : -80.87\n",
      "Episode : 316 \t\t Timestep : 32000 \t\t Average Reward : -16.85\n",
      "Episode : 326 \t\t Timestep : 33600 \t\t Average Reward : -27.31\n",
      "Episode : 337 \t\t Timestep : 35200 \t\t Average Reward : -39.03\n",
      "Episode : 346 \t\t Timestep : 36800 \t\t Average Reward : -21.99\n",
      "Episode : 355 \t\t Timestep : 38400 \t\t Average Reward : -49.49\n",
      "Episode : 364 \t\t Timestep : 40000 \t\t Average Reward : 5.63\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:11\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 372 \t\t Timestep : 41600 \t\t Average Reward : 42.85\n",
      "Episode : 380 \t\t Timestep : 43200 \t\t Average Reward : -25.8\n",
      "Episode : 387 \t\t Timestep : 44800 \t\t Average Reward : 25.3\n",
      "Episode : 391 \t\t Timestep : 46400 \t\t Average Reward : -54.59\n",
      "Episode : 395 \t\t Timestep : 48000 \t\t Average Reward : -5.17\n",
      "Episode : 401 \t\t Timestep : 49600 \t\t Average Reward : 48.4\n",
      "Episode : 405 \t\t Timestep : 51200 \t\t Average Reward : 33.36\n",
      "Episode : 409 \t\t Timestep : 52800 \t\t Average Reward : 83.17\n",
      "Episode : 415 \t\t Timestep : 54400 \t\t Average Reward : 81.92\n",
      "Episode : 423 \t\t Timestep : 56000 \t\t Average Reward : -17.59\n",
      "Episode : 431 \t\t Timestep : 57600 \t\t Average Reward : 19.88\n",
      "Episode : 441 \t\t Timestep : 59200 \t\t Average Reward : 14.72\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:01:56\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 446 \t\t Timestep : 60800 \t\t Average Reward : 69.83\n",
      "Episode : 450 \t\t Timestep : 62400 \t\t Average Reward : 72.93\n",
      "Episode : 456 \t\t Timestep : 64000 \t\t Average Reward : 66.93\n",
      "Episode : 460 \t\t Timestep : 65600 \t\t Average Reward : 89.2\n",
      "Episode : 466 \t\t Timestep : 67200 \t\t Average Reward : 23.03\n",
      "Episode : 470 \t\t Timestep : 68800 \t\t Average Reward : 101.31\n",
      "Episode : 477 \t\t Timestep : 70400 \t\t Average Reward : 31.49\n",
      "Episode : 481 \t\t Timestep : 72000 \t\t Average Reward : 119.83\n",
      "Episode : 486 \t\t Timestep : 73600 \t\t Average Reward : 101.1\n",
      "Episode : 490 \t\t Timestep : 75200 \t\t Average Reward : 128.36\n",
      "Episode : 494 \t\t Timestep : 76800 \t\t Average Reward : 157.45\n",
      "Episode : 499 \t\t Timestep : 78400 \t\t Average Reward : 69.86\n",
      "Episode : 506 \t\t Timestep : 80000 \t\t Average Reward : 86.51\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:02:43\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 513 \t\t Timestep : 81600 \t\t Average Reward : 28.72\n",
      "Episode : 520 \t\t Timestep : 83200 \t\t Average Reward : 77.12\n",
      "Episode : 526 \t\t Timestep : 84800 \t\t Average Reward : 85.69\n",
      "Episode : 532 \t\t Timestep : 86400 \t\t Average Reward : 72.51\n",
      "Episode : 538 \t\t Timestep : 88000 \t\t Average Reward : 86.05\n",
      "Episode : 542 \t\t Timestep : 89600 \t\t Average Reward : 169.7\n",
      "Episode : 548 \t\t Timestep : 91200 \t\t Average Reward : 94.62\n",
      "Episode : 554 \t\t Timestep : 92800 \t\t Average Reward : 94.07\n",
      "Episode : 559 \t\t Timestep : 94400 \t\t Average Reward : 89.2\n",
      "Episode : 564 \t\t Timestep : 96000 \t\t Average Reward : 117.66\n",
      "Episode : 569 \t\t Timestep : 97600 \t\t Average Reward : 88.76\n",
      "Episode : 574 \t\t Timestep : 99200 \t\t Average Reward : 103.52\n",
      "--------------------------------------------------------------------------------------------\n",
      "saving model at : PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth\n",
      "model saved\n",
      "Elapsed Time  :  0:03:28\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "while time_step <= max_training_timesteps:\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        \n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            \n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "\n",
    "\n",
    "log_f.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
